# 6.27日报

## speech-enhancement

1. 写脚本数据清洗，基于开源数据集进行降采样(减少gpu占用)，降低数据集大小；之后对语音信号归一化，防止语音叠加后溢出被截断；最后将noisy与clean数据集叠加为mixed数据，共100000条wav文件在训练集、1000条wav文件在测试集。
2. 更换模型，基于Grad-RNN模型进行优化，思路是加上dual path作为一维数据的时域位置对照组，恢复编码位置。
3. 将前半段的DNN结构直接简化，取消fc层，加入1x1conv层降低采样通道数，有限硬件条件下支持8线程并行计算。
4. 最终torchsummary打印参数量仅仅只有23.67k，相比RNNoise等模型少了近10倍。
5. 训练模型，得到pth文件371kb，onnx文件373kb。转换为coreml文件6.0MB，mlpackage文件4.9MB，便于部署到iOS设备上。

## 下一步

1. 撰写该模型的分析报告，之后将具体从参数对比、wav时频谱图、以及听力直观感受方面汇总完整的汇报，本周应该可以提交微盘共享链接。
2. 对于speech-enhancement，换数据集继续跑，生成多组对照。
