# 7.2日

## speech-enhancement infer

1. 为不同的语音增强模型写推理脚本。处理对象是10s以内的音频，模型包括：
2. denoiser：效果最佳，模型体积134MB，对48kHz的音频去噪增强效果最佳，其中预训练模型种类多样，针对不同的采样率与位深有不同的模型大小，最后选用64k参数量的模型大小为134MB。
3. DPCRN：基于pytorch框架写stream卷积时SDR下降的很剧烈，但是基于原生tf框架下写推理，效果就提升的很明显，估计是正向传播过程没有考虑cache的叠加缓存。
4. SEGAN+：模型最大，达到了245MB，效果也可以，相较于传统降噪多了一层mask增强的效果。
5. DPCRN-Quant：模型最小。处理时需要对48khz信号降采样为16khz，在输入模型。在blind test中效果很差，但是dns3中推理效果很好。
6. DeepFilter：明天继续写，real to time的推理结果不是很理想。

## 解决了两个困扰的报错

1. 今天在推理LSTM模型时，出现了报错：`TypeError: 'NoneType' object is not subscriptable`，原因是`self._train_op = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)`中的`self.loss`为`None`，在`self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits, labels=self.y))`中，`self.logits`为`None`，在`self.logits = tf.layers.dense(self.h_pool_flat, self.num_classes)`中，`self.h_pool_flat`为`None`，在`self.h_pool_flat = tf.reshape(self.h_pool, [-1, self.h_pool.shape[1] * self.h_pool.shape[2] * self.h_pool.shape[3]])`中，`self.h_pool`为`None`，在`self.h_pool = tf.nn.max_pool(self.h_conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')`中，`self.h_conv3`为`None`
2. 解决方法：在`self.h_pool_flat = tf.reshape(self.h_pool, [-1, self.h_pool.shape[1] * self.h_pool.shape[2] * self.h_pool.shape[3]])`中，`self.h_pool`为`None`，在`self.h_pool = tf.nn.max_pool(self.h_conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')`中，`self.h_conv3`为`None`，在`self.h_conv3 = tf.nn.relu(tf.nn.conv2d(self.h_conv2, self.w_conv3, strides=[1, 1, 1, 1], padding='SAME') + self.b_conv3)`中，`self.w_conv3`为`None`，在`self.w_conv3 = tf.Variable(tf.truncated_normal([3, 3, 64, 128], stddev=0.1))`中，`self.w_conv3`为`None`，在`self.b_conv3 = tf.Variable(tf.constant(0.1, shape=[128]))`中，`self.b_conv3`
3. STFT与ISTFT的区别：在torch的不同版本，对于结果的实部和虚部，istft会有不同的需求。在>=2.0.0版本中，`return_complex=True`，返回的结果是复数，`return_complex=False`，返回的结果是实数。在<2.0.0版本中，没有return的强制限制。推荐版本：`torch==1.11.0`,`torchvision==0.12.0.`,`torchaudio==0.11.0`
4. 注意permute与transpose两个转置函数的区别。`input.shape`与`len(dim)`会不匹配

## 下一步

1. 寻找speech-enhancement模型推理失败的原因，从checkpoint的具体位置重新迁移学习，排除是否是偶然情况。按照demo结果，SDR起码可以达到10上下。
2. 对于speech-enhancement，再找找均衡的模型继续跑，生成多组对照。
3. 情绪分析用CREME数据集，继续扩大模型，测试准确率。
